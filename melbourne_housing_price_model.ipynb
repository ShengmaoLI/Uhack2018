{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load library\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import optimizers\n",
    "from keras.utils import plot_model\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import pydot\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Dataset\n",
    "dataset = pd.read_csv(\"dataset/Melbourne_housing_FULL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove rows with null value.\n",
    "dataset_rn = dataset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize y to ensure the loss will become very large\n",
    "y= dataset_rn.iloc[:,[4]]/1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select targe features from datasets\n",
    "x = dataset_rn.iloc[:,[0,2,3,5,8,10,11,12,13,14,15]]\n",
    "#One hot encoding to transfer columns with categorical values for neural network training.\n",
    "x_dummies = pd.get_dummies(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset into training part and test part. Due to the time limitations, we don't set training dataset,\n",
    "#dev dataset, and test dataset. Just training dataset and test dataset.\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x_dummies,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Design the model of neural networks. Since this project is to predict the numeric values as output, we use\n",
    "#regression rather than binary.\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=331, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since it is not binary result, for loss we use Mean Squared Error(MSE) as the loss function, optimizer is set as adam\n",
    "#rather than gradient descent. Metrics is MSE as well.\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set starting time\n",
    "t1 = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "7109/7109 [==============================] - 5s 712us/step - loss: 1.0321 - mean_squared_error: 1.0321\n",
      "Epoch 2/150\n",
      "7109/7109 [==============================] - 3s 412us/step - loss: 0.2945 - mean_squared_error: 0.2945\n",
      "Epoch 3/150\n",
      "7109/7109 [==============================] - 3s 411us/step - loss: 0.2718 - mean_squared_error: 0.2718\n",
      "Epoch 4/150\n",
      "7109/7109 [==============================] - 3s 387us/step - loss: 0.2459 - mean_squared_error: 0.2459\n",
      "Epoch 5/150\n",
      "7109/7109 [==============================] - 3s 361us/step - loss: 0.2410 - mean_squared_error: 0.2410\n",
      "Epoch 6/150\n",
      "7109/7109 [==============================] - 2s 345us/step - loss: 0.2213 - mean_squared_error: 0.2213\n",
      "Epoch 7/150\n",
      "7109/7109 [==============================] - 3s 355us/step - loss: 0.2276 - mean_squared_error: 0.2276\n",
      "Epoch 8/150\n",
      "7109/7109 [==============================] - 3s 369us/step - loss: 0.2537 - mean_squared_error: 0.2537\n",
      "Epoch 9/150\n",
      "7109/7109 [==============================] - 3s 390us/step - loss: 0.1945 - mean_squared_error: 0.1945\n",
      "Epoch 10/150\n",
      "7109/7109 [==============================] - 3s 354us/step - loss: 0.1858 - mean_squared_error: 0.1858\n",
      "Epoch 11/150\n",
      "7109/7109 [==============================] - 3s 356us/step - loss: 0.1830 - mean_squared_error: 0.1830\n",
      "Epoch 12/150\n",
      "7109/7109 [==============================] - 2s 348us/step - loss: 0.1752 - mean_squared_error: 0.1752\n",
      "Epoch 13/150\n",
      "7109/7109 [==============================] - 3s 368us/step - loss: 0.1741 - mean_squared_error: 0.1741\n",
      "Epoch 14/150\n",
      "7109/7109 [==============================] - 3s 358us/step - loss: 0.1701 - mean_squared_error: 0.1701\n",
      "Epoch 15/150\n",
      "7109/7109 [==============================] - 2s 327us/step - loss: 0.1718 - mean_squared_error: 0.1718\n",
      "Epoch 16/150\n",
      "7109/7109 [==============================] - 3s 365us/step - loss: 0.1732 - mean_squared_error: 0.1732\n",
      "Epoch 17/150\n",
      "7109/7109 [==============================] - 3s 376us/step - loss: 0.1641 - mean_squared_error: 0.1641\n",
      "Epoch 18/150\n",
      "7109/7109 [==============================] - 3s 366us/step - loss: 0.1634 - mean_squared_error: 0.1634\n",
      "Epoch 19/150\n",
      "7109/7109 [==============================] - 3s 362us/step - loss: 0.1610 - mean_squared_error: 0.1610\n",
      "Epoch 20/150\n",
      "7109/7109 [==============================] - 3s 372us/step - loss: 0.1594 - mean_squared_error: 0.1594\n",
      "Epoch 21/150\n",
      "7109/7109 [==============================] - 2s 344us/step - loss: 0.1558 - mean_squared_error: 0.1558\n",
      "Epoch 22/150\n",
      "7109/7109 [==============================] - 2s 337us/step - loss: 0.1625 - mean_squared_error: 0.1625\n",
      "Epoch 23/150\n",
      "7109/7109 [==============================] - 2s 344us/step - loss: 0.1535 - mean_squared_error: 0.1535\n",
      "Epoch 24/150\n",
      "7109/7109 [==============================] - 3s 352us/step - loss: 0.1475 - mean_squared_error: 0.1475\n",
      "Epoch 25/150\n",
      "7109/7109 [==============================] - 2s 322us/step - loss: 0.1576 - mean_squared_error: 0.1576\n",
      "Epoch 26/150\n",
      "7109/7109 [==============================] - 2s 351us/step - loss: 0.1522 - mean_squared_error: 0.1522\n",
      "Epoch 27/150\n",
      "7109/7109 [==============================] - 3s 403us/step - loss: 0.1446 - mean_squared_error: 0.1446\n",
      "Epoch 28/150\n",
      "7109/7109 [==============================] - 3s 379us/step - loss: 0.1454 - mean_squared_error: 0.1454\n",
      "Epoch 29/150\n",
      "7109/7109 [==============================] - 2s 348us/step - loss: 0.1398 - mean_squared_error: 0.1398\n",
      "Epoch 30/150\n",
      "7109/7109 [==============================] - 2s 342us/step - loss: 0.1444 - mean_squared_error: 0.1444\n",
      "Epoch 31/150\n",
      "7109/7109 [==============================] - 2s 316us/step - loss: 0.1348 - mean_squared_error: 0.1348\n",
      "Epoch 32/150\n",
      "7109/7109 [==============================] - 2s 305us/step - loss: 0.1341 - mean_squared_error: 0.1341\n",
      "Epoch 33/150\n",
      "7109/7109 [==============================] - 2s 303us/step - loss: 0.1362 - mean_squared_error: 0.1362\n",
      "Epoch 34/150\n",
      "7109/7109 [==============================] - 2s 316us/step - loss: 0.1308 - mean_squared_error: 0.1308\n",
      "Epoch 35/150\n",
      "7109/7109 [==============================] - 2s 307us/step - loss: 0.1299 - mean_squared_error: 0.1299\n",
      "Epoch 36/150\n",
      "7109/7109 [==============================] - 2s 305us/step - loss: 0.1267 - mean_squared_error: 0.1267\n",
      "Epoch 37/150\n",
      "7109/7109 [==============================] - 2s 320us/step - loss: 0.1239 - mean_squared_error: 0.1239\n",
      "Epoch 38/150\n",
      "7109/7109 [==============================] - 2s 302us/step - loss: 0.1239 - mean_squared_error: 0.1239\n",
      "Epoch 39/150\n",
      "7109/7109 [==============================] - 2s 315us/step - loss: 0.1258 - mean_squared_error: 0.1258\n",
      "Epoch 40/150\n",
      "7109/7109 [==============================] - 2s 302us/step - loss: 0.1231 - mean_squared_error: 0.1231\n",
      "Epoch 41/150\n",
      "7109/7109 [==============================] - 2s 321us/step - loss: 0.1412 - mean_squared_error: 0.1412\n",
      "Epoch 42/150\n",
      "7109/7109 [==============================] - 2s 336us/step - loss: 0.1167 - mean_squared_error: 0.1167\n",
      "Epoch 43/150\n",
      "7109/7109 [==============================] - 2s 308us/step - loss: 0.1209 - mean_squared_error: 0.1209\n",
      "Epoch 44/150\n",
      "7109/7109 [==============================] - 2s 349us/step - loss: 0.1247 - mean_squared_error: 0.1247\n",
      "Epoch 45/150\n",
      "7109/7109 [==============================] - 2s 316us/step - loss: 0.1171 - mean_squared_error: 0.1171\n",
      "Epoch 46/150\n",
      "7109/7109 [==============================] - 2s 330us/step - loss: 0.1125 - mean_squared_error: 0.1125\n",
      "Epoch 47/150\n",
      "7109/7109 [==============================] - 2s 310us/step - loss: 0.1142 - mean_squared_error: 0.1142\n",
      "Epoch 48/150\n",
      "7109/7109 [==============================] - 2s 314us/step - loss: 0.1165 - mean_squared_error: 0.1165\n",
      "Epoch 49/150\n",
      "7109/7109 [==============================] - 2s 313us/step - loss: 0.1193 - mean_squared_error: 0.1193\n",
      "Epoch 50/150\n",
      "7109/7109 [==============================] - 2s 314us/step - loss: 0.1161 - mean_squared_error: 0.1161\n",
      "Epoch 51/150\n",
      "7109/7109 [==============================] - 3s 369us/step - loss: 0.1248 - mean_squared_error: 0.1248\n",
      "Epoch 52/150\n",
      "7109/7109 [==============================] - 2s 315us/step - loss: 0.1179 - mean_squared_error: 0.1179\n",
      "Epoch 53/150\n",
      "7109/7109 [==============================] - 2s 350us/step - loss: 0.1156 - mean_squared_error: 0.1156\n",
      "Epoch 54/150\n",
      "7109/7109 [==============================] - 2s 317us/step - loss: 0.1189 - mean_squared_error: 0.1189\n",
      "Epoch 55/150\n",
      "7109/7109 [==============================] - 2s 295us/step - loss: 0.1103 - mean_squared_error: 0.1103\n",
      "Epoch 56/150\n",
      "7109/7109 [==============================] - 2s 314us/step - loss: 0.1085 - mean_squared_error: 0.1085\n",
      "Epoch 57/150\n",
      "7109/7109 [==============================] - 2s 328us/step - loss: 0.1077 - mean_squared_error: 0.1077\n",
      "Epoch 58/150\n",
      "7109/7109 [==============================] - 2s 297us/step - loss: 0.1083 - mean_squared_error: 0.1083\n",
      "Epoch 59/150\n",
      "7109/7109 [==============================] - 2s 309us/step - loss: 0.1113 - mean_squared_error: 0.1113\n",
      "Epoch 60/150\n",
      "7109/7109 [==============================] - 2s 296us/step - loss: 0.1040 - mean_squared_error: 0.1040\n",
      "Epoch 61/150\n",
      "7109/7109 [==============================] - 2s 311us/step - loss: 0.1102 - mean_squared_error: 0.1102\n",
      "Epoch 62/150\n",
      "7109/7109 [==============================] - 2s 313us/step - loss: 0.1065 - mean_squared_error: 0.1065\n",
      "Epoch 63/150\n",
      "7109/7109 [==============================] - 2s 314us/step - loss: 0.1069 - mean_squared_error: 0.1069\n",
      "Epoch 64/150\n",
      "7109/7109 [==============================] - 2s 310us/step - loss: 0.1017 - mean_squared_error: 0.1017\n",
      "Epoch 65/150\n",
      "7109/7109 [==============================] - 2s 309us/step - loss: 0.1096 - mean_squared_error: 0.1096\n",
      "Epoch 66/150\n",
      "7109/7109 [==============================] - 2s 294us/step - loss: 0.1010 - mean_squared_error: 0.1010\n",
      "Epoch 67/150\n",
      "7109/7109 [==============================] - 2s 308us/step - loss: 0.1039 - mean_squared_error: 0.1039\n",
      "Epoch 68/150\n",
      "7109/7109 [==============================] - 2s 314us/step - loss: 0.0981 - mean_squared_error: 0.0981\n",
      "Epoch 69/150\n",
      "7109/7109 [==============================] - 2s 313us/step - loss: 0.1002 - mean_squared_error: 0.1002\n",
      "Epoch 70/150\n",
      "7109/7109 [==============================] - 2s 310us/step - loss: 0.1024 - mean_squared_error: 0.1024\n",
      "Epoch 71/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7109/7109 [==============================] - 2s 292us/step - loss: 0.0945 - mean_squared_error: 0.0945\n",
      "Epoch 72/150\n",
      "7109/7109 [==============================] - 2s 291us/step - loss: 0.1008 - mean_squared_error: 0.1008\n",
      "Epoch 73/150\n",
      "7109/7109 [==============================] - 2s 292us/step - loss: 0.1023 - mean_squared_error: 0.1023\n",
      "Epoch 74/150\n",
      "7109/7109 [==============================] - 2s 291us/step - loss: 0.1003 - mean_squared_error: 0.1003\n",
      "Epoch 75/150\n",
      "7109/7109 [==============================] - 2s 306us/step - loss: 0.1042 - mean_squared_error: 0.1042\n",
      "Epoch 76/150\n",
      "7109/7109 [==============================] - 2s 310us/step - loss: 0.0988 - mean_squared_error: 0.0988\n",
      "Epoch 77/150\n",
      "7109/7109 [==============================] - 2s 316us/step - loss: 0.0983 - mean_squared_error: 0.0983\n",
      "Epoch 78/150\n",
      "7109/7109 [==============================] - 2s 293us/step - loss: 0.0945 - mean_squared_error: 0.0945\n",
      "Epoch 79/150\n",
      "7109/7109 [==============================] - 2s 316us/step - loss: 0.1062 - mean_squared_error: 0.1062\n",
      "Epoch 80/150\n",
      "7109/7109 [==============================] - 2s 300us/step - loss: 0.0934 - mean_squared_error: 0.0934\n",
      "Epoch 81/150\n",
      "7109/7109 [==============================] - 2s 336us/step - loss: 0.0916 - mean_squared_error: 0.0916\n",
      "Epoch 82/150\n",
      "7109/7109 [==============================] - 2s 332us/step - loss: 0.0978 - mean_squared_error: 0.0978\n",
      "Epoch 83/150\n",
      "7109/7109 [==============================] - 2s 310us/step - loss: 0.1050 - mean_squared_error: 0.1050\n",
      "Epoch 84/150\n",
      "7109/7109 [==============================] - 3s 360us/step - loss: 0.0907 - mean_squared_error: 0.0907\n",
      "Epoch 85/150\n",
      "7109/7109 [==============================] - 2s 317us/step - loss: 0.0990 - mean_squared_error: 0.0990\n",
      "Epoch 86/150\n",
      "7109/7109 [==============================] - 2s 301us/step - loss: 0.0921 - mean_squared_error: 0.0921\n",
      "Epoch 87/150\n",
      "7109/7109 [==============================] - 2s 315us/step - loss: 0.1019 - mean_squared_error: 0.1019\n",
      "Epoch 88/150\n",
      "7109/7109 [==============================] - 2s 324us/step - loss: 0.0956 - mean_squared_error: 0.0956\n",
      "Epoch 89/150\n",
      "7109/7109 [==============================] - 2s 328us/step - loss: 0.0968 - mean_squared_error: 0.0968\n",
      "Epoch 90/150\n",
      "7109/7109 [==============================] - 2s 300us/step - loss: 0.0933 - mean_squared_error: 0.0933\n",
      "Epoch 91/150\n",
      "7109/7109 [==============================] - 2s 301us/step - loss: 0.0891 - mean_squared_error: 0.0891\n",
      "Epoch 92/150\n",
      "7109/7109 [==============================] - 2s 334us/step - loss: 0.0966 - mean_squared_error: 0.0966\n",
      "Epoch 93/150\n",
      "7109/7109 [==============================] - 2s 297us/step - loss: 0.0898 - mean_squared_error: 0.0898\n",
      "Epoch 94/150\n",
      "7109/7109 [==============================] - 2s 298us/step - loss: 0.0942 - mean_squared_error: 0.0942\n",
      "Epoch 95/150\n",
      "7109/7109 [==============================] - 2s 297us/step - loss: 0.0884 - mean_squared_error: 0.0884\n",
      "Epoch 96/150\n",
      "7109/7109 [==============================] - 2s 324us/step - loss: 0.0938 - mean_squared_error: 0.0938\n",
      "Epoch 97/150\n",
      "7109/7109 [==============================] - 2s 339us/step - loss: 0.0929 - mean_squared_error: 0.0929\n",
      "Epoch 98/150\n",
      "7109/7109 [==============================] - 2s 299us/step - loss: 0.0906 - mean_squared_error: 0.0906\n",
      "Epoch 99/150\n",
      "7109/7109 [==============================] - 2s 322us/step - loss: 0.0893 - mean_squared_error: 0.0893\n",
      "Epoch 100/150\n",
      "7109/7109 [==============================] - 2s 314us/step - loss: 0.0912 - mean_squared_error: 0.0912\n",
      "Epoch 101/150\n",
      "7109/7109 [==============================] - 2s 302us/step - loss: 0.0947 - mean_squared_error: 0.0947\n",
      "Epoch 102/150\n",
      "7109/7109 [==============================] - 2s 349us/step - loss: 0.0893 - mean_squared_error: 0.0893\n",
      "Epoch 103/150\n",
      "7109/7109 [==============================] - 2s 294us/step - loss: 0.0893 - mean_squared_error: 0.0893\n",
      "Epoch 104/150\n",
      "7109/7109 [==============================] - 2s 312us/step - loss: 0.0972 - mean_squared_error: 0.0972\n",
      "Epoch 105/150\n",
      "7109/7109 [==============================] - 2s 299us/step - loss: 0.0879 - mean_squared_error: 0.0879\n",
      "Epoch 106/150\n",
      "7109/7109 [==============================] - 2s 295us/step - loss: 0.0858 - mean_squared_error: 0.0858\n",
      "Epoch 107/150\n",
      "7109/7109 [==============================] - 2s 327us/step - loss: 0.0907 - mean_squared_error: 0.0907\n",
      "Epoch 108/150\n",
      "7109/7109 [==============================] - 2s 292us/step - loss: 0.0946 - mean_squared_error: 0.0946\n",
      "Epoch 109/150\n",
      "7109/7109 [==============================] - 2s 326us/step - loss: 0.0865 - mean_squared_error: 0.0865\n",
      "Epoch 110/150\n",
      "7109/7109 [==============================] - 2s 306us/step - loss: 0.0872 - mean_squared_error: 0.0872\n",
      "Epoch 111/150\n",
      "7109/7109 [==============================] - 2s 333us/step - loss: 0.0867 - mean_squared_error: 0.0867\n",
      "Epoch 112/150\n",
      "7109/7109 [==============================] - 2s 312us/step - loss: 0.0899 - mean_squared_error: 0.0899\n",
      "Epoch 113/150\n",
      "7109/7109 [==============================] - 2s 307us/step - loss: 0.0849 - mean_squared_error: 0.0849\n",
      "Epoch 114/150\n",
      "7109/7109 [==============================] - 2s 342us/step - loss: 0.0870 - mean_squared_error: 0.0870\n",
      "Epoch 115/150\n",
      "7109/7109 [==============================] - 2s 314us/step - loss: 0.0988 - mean_squared_error: 0.0988\n",
      "Epoch 116/150\n",
      "7109/7109 [==============================] - 2s 323us/step - loss: 0.0847 - mean_squared_error: 0.0847\n",
      "Epoch 117/150\n",
      "7109/7109 [==============================] - 2s 297us/step - loss: 0.0868 - mean_squared_error: 0.0868\n",
      "Epoch 118/150\n",
      "7109/7109 [==============================] - 2s 319us/step - loss: 0.0892 - mean_squared_error: 0.0892\n",
      "Epoch 119/150\n",
      "7109/7109 [==============================] - 2s 305us/step - loss: 0.0895 - mean_squared_error: 0.0895\n",
      "Epoch 120/150\n",
      "7109/7109 [==============================] - 3s 368us/step - loss: 0.0929 - mean_squared_error: 0.0929\n",
      "Epoch 121/150\n",
      "7109/7109 [==============================] - 3s 385us/step - loss: 0.0857 - mean_squared_error: 0.0857\n",
      "Epoch 122/150\n",
      "7109/7109 [==============================] - 3s 407us/step - loss: 0.0891 - mean_squared_error: 0.0891\n",
      "Epoch 123/150\n",
      "7109/7109 [==============================] - 2s 339us/step - loss: 0.0867 - mean_squared_error: 0.0867\n",
      "Epoch 124/150\n",
      "7109/7109 [==============================] - 2s 343us/step - loss: 0.0952 - mean_squared_error: 0.0952\n",
      "Epoch 125/150\n",
      "7109/7109 [==============================] - 2s 304us/step - loss: 0.0904 - mean_squared_error: 0.0904\n",
      "Epoch 126/150\n",
      "7109/7109 [==============================] - 2s 302us/step - loss: 0.0894 - mean_squared_error: 0.0894\n",
      "Epoch 127/150\n",
      "7109/7109 [==============================] - 2s 299us/step - loss: 0.0874 - mean_squared_error: 0.0874\n",
      "Epoch 128/150\n",
      "7109/7109 [==============================] - 2s 301us/step - loss: 0.0917 - mean_squared_error: 0.0917\n",
      "Epoch 129/150\n",
      "7109/7109 [==============================] - 2s 328us/step - loss: 0.0823 - mean_squared_error: 0.0823\n",
      "Epoch 130/150\n",
      "7109/7109 [==============================] - 2s 313us/step - loss: 0.0873 - mean_squared_error: 0.0873\n",
      "Epoch 131/150\n",
      "7109/7109 [==============================] - 2s 299us/step - loss: 0.0842 - mean_squared_error: 0.0842\n",
      "Epoch 132/150\n",
      "7109/7109 [==============================] - 2s 326us/step - loss: 0.0851 - mean_squared_error: 0.0851\n",
      "Epoch 133/150\n",
      "7109/7109 [==============================] - 3s 399us/step - loss: 0.0853 - mean_squared_error: 0.0853\n",
      "Epoch 134/150\n",
      "7109/7109 [==============================] - 2s 312us/step - loss: 0.0858 - mean_squared_error: 0.0858\n",
      "Epoch 135/150\n",
      "7109/7109 [==============================] - 2s 334us/step - loss: 0.0859 - mean_squared_error: 0.0859\n",
      "Epoch 136/150\n",
      "7109/7109 [==============================] - 2s 313us/step - loss: 0.0839 - mean_squared_error: 0.0839\n",
      "Epoch 137/150\n",
      "7109/7109 [==============================] - 2s 333us/step - loss: 0.0856 - mean_squared_error: 0.0856\n",
      "Epoch 138/150\n",
      "7109/7109 [==============================] - 3s 389us/step - loss: 0.0822 - mean_squared_error: 0.0822\n",
      "Epoch 139/150\n",
      "7109/7109 [==============================] - 2s 336us/step - loss: 0.0863 - mean_squared_error: 0.0863\n",
      "Epoch 140/150\n",
      "7109/7109 [==============================] - 2s 326us/step - loss: 0.0862 - mean_squared_error: 0.0862\n",
      "Epoch 141/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7109/7109 [==============================] - 2s 314us/step - loss: 0.0842 - mean_squared_error: 0.0842\n",
      "Epoch 142/150\n",
      "7109/7109 [==============================] - 2s 315us/step - loss: 0.0838 - mean_squared_error: 0.0838\n",
      "Epoch 143/150\n",
      "7109/7109 [==============================] - 2s 298us/step - loss: 0.0871 - mean_squared_error: 0.0871\n",
      "Epoch 144/150\n",
      "7109/7109 [==============================] - 2s 296us/step - loss: 0.0843 - mean_squared_error: 0.0843\n",
      "Epoch 145/150\n",
      "7109/7109 [==============================] - 2s 312us/step - loss: 0.0821 - mean_squared_error: 0.0821\n",
      "Epoch 146/150\n",
      "7109/7109 [==============================] - 2s 298us/step - loss: 0.0826 - mean_squared_error: 0.0826\n",
      "Epoch 147/150\n",
      "7109/7109 [==============================] - 2s 330us/step - loss: 0.0858 - mean_squared_error: 0.0858\n",
      "Epoch 148/150\n",
      "7109/7109 [==============================] - 3s 356us/step - loss: 0.0867 - mean_squared_error: 0.0867\n",
      "Epoch 149/150\n",
      "7109/7109 [==============================] - 2s 316us/step - loss: 0.0921 - mean_squared_error: 0.0921\n",
      "Epoch 150/150\n",
      "7109/7109 [==============================] - 2s 323us/step - loss: 0.0810 - mean_squared_error: 0.0810\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a318e98d0>"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run training\n",
    "model.fit(xtrain, ytrain, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:08:00.836613\n"
     ]
    }
   ],
   "source": [
    "#Set ending time \n",
    "t2 = datetime.datetime.now()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use test dataset to verify the output of predictions. \n",
    "predictions = np.multiply(model.predict(xtest),1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1778/1778 [==============================] - 2s 974us/step\n",
      "\n",
      "Loss = 0.07644647805709538\n",
      "Mean Squared Error = 0.07644647805709538\n"
     ]
    }
   ],
   "source": [
    "preds = model.evaluate(xtest,ytest)\n",
    "print()\n",
    "print(\"Loss = \" + str(preds[0]))\n",
    "print(\"Mean Squared Error = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.045038e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.153394e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.284247e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.799180e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.811018e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.791363e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.496080e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.143506e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.016377e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.933409e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.278216e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.766171e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.558968e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.509351e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.542937e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9.599345e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.927913e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.638133e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3.405822e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4.006737e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4.266463e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4.753832e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.509896e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.200601e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.326508e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.384602e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.210212e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8.112944e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.363712e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5.068355e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>6.862495e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749</th>\n",
       "      <td>1.592497e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1750</th>\n",
       "      <td>1.481345e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751</th>\n",
       "      <td>8.449089e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1752</th>\n",
       "      <td>6.299347e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1753</th>\n",
       "      <td>1.384770e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754</th>\n",
       "      <td>6.577606e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1755</th>\n",
       "      <td>1.497402e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1756</th>\n",
       "      <td>1.689961e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1757</th>\n",
       "      <td>1.572213e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1758</th>\n",
       "      <td>8.199893e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1759</th>\n",
       "      <td>2.794320e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1760</th>\n",
       "      <td>1.664271e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1761</th>\n",
       "      <td>8.107780e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1762</th>\n",
       "      <td>2.296443e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1763</th>\n",
       "      <td>5.077583e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764</th>\n",
       "      <td>1.174229e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1765</th>\n",
       "      <td>1.244449e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766</th>\n",
       "      <td>1.373222e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1767</th>\n",
       "      <td>1.665722e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768</th>\n",
       "      <td>6.716301e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>4.452849e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1770</th>\n",
       "      <td>3.145469e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1771</th>\n",
       "      <td>1.575067e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1772</th>\n",
       "      <td>5.429810e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1773</th>\n",
       "      <td>4.448858e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1774</th>\n",
       "      <td>8.561917e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1775</th>\n",
       "      <td>6.755225e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1776</th>\n",
       "      <td>1.495997e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1777</th>\n",
       "      <td>7.299235e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1778 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "0     1.045038e+06\n",
       "1     1.153394e+06\n",
       "2     2.284247e+06\n",
       "3     4.799180e+05\n",
       "4     2.811018e+06\n",
       "5     6.791363e+05\n",
       "6     7.496080e+05\n",
       "7     1.143506e+06\n",
       "8     1.016377e+06\n",
       "9     3.933409e+05\n",
       "10    2.278216e+06\n",
       "11    2.766171e+06\n",
       "12    1.558968e+06\n",
       "13    1.509351e+06\n",
       "14    1.542937e+06\n",
       "15    9.599345e+05\n",
       "16    4.927913e+05\n",
       "17    4.638133e+06\n",
       "18    3.405822e+06\n",
       "19    4.006737e+05\n",
       "20    4.266463e+05\n",
       "21    4.753832e+05\n",
       "22    1.509896e+06\n",
       "23    5.200601e+05\n",
       "24    1.326508e+06\n",
       "25    1.384602e+06\n",
       "26    1.210212e+06\n",
       "27    8.112944e+05\n",
       "28    1.363712e+06\n",
       "29    5.068355e+05\n",
       "...            ...\n",
       "1748  6.862495e+05\n",
       "1749  1.592497e+06\n",
       "1750  1.481345e+06\n",
       "1751  8.449089e+05\n",
       "1752  6.299347e+05\n",
       "1753  1.384770e+06\n",
       "1754  6.577606e+05\n",
       "1755  1.497402e+06\n",
       "1756  1.689961e+06\n",
       "1757  1.572213e+06\n",
       "1758  8.199893e+05\n",
       "1759  2.794320e+06\n",
       "1760  1.664271e+06\n",
       "1761  8.107780e+05\n",
       "1762  2.296443e+06\n",
       "1763  5.077583e+05\n",
       "1764  1.174229e+06\n",
       "1765  1.244449e+06\n",
       "1766  1.373222e+06\n",
       "1767  1.665722e+06\n",
       "1768  6.716301e+05\n",
       "1769  4.452849e+05\n",
       "1770  3.145469e+06\n",
       "1771  1.575067e+06\n",
       "1772  5.429810e+05\n",
       "1773  4.448858e+06\n",
       "1774  8.561917e+05\n",
       "1775  6.755225e+05\n",
       "1776  1.495997e+06\n",
       "1777  7.299235e+05\n",
       "\n",
       "[1778 rows x 1 columns]"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display result of predictions\n",
    "pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2367</th>\n",
       "      <td>1330000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20408</th>\n",
       "      <td>980000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19819</th>\n",
       "      <td>2515000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8833</th>\n",
       "      <td>680000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1753</th>\n",
       "      <td>2905000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>706750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20901</th>\n",
       "      <td>730000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9831</th>\n",
       "      <td>910000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34542</th>\n",
       "      <td>922000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10782</th>\n",
       "      <td>501000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18844</th>\n",
       "      <td>2400000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10383</th>\n",
       "      <td>1435000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29844</th>\n",
       "      <td>1700000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2215</th>\n",
       "      <td>1800000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24725</th>\n",
       "      <td>1310000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9460</th>\n",
       "      <td>1016000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5135</th>\n",
       "      <td>400000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26147</th>\n",
       "      <td>6125000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1884</th>\n",
       "      <td>3900000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14220</th>\n",
       "      <td>360000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12579</th>\n",
       "      <td>305000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9067</th>\n",
       "      <td>450000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32702</th>\n",
       "      <td>1300000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3035</th>\n",
       "      <td>585000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7632</th>\n",
       "      <td>1196000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18398</th>\n",
       "      <td>1420000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10816</th>\n",
       "      <td>1350000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18716</th>\n",
       "      <td>755000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15331</th>\n",
       "      <td>1350000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>580000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6163</th>\n",
       "      <td>490000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10961</th>\n",
       "      <td>1450000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3101</th>\n",
       "      <td>1577500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3514</th>\n",
       "      <td>1172000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12138</th>\n",
       "      <td>480000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>1415000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17636</th>\n",
       "      <td>450000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2122</th>\n",
       "      <td>1813000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27330</th>\n",
       "      <td>1560000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17791</th>\n",
       "      <td>1635000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32161</th>\n",
       "      <td>856000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26146</th>\n",
       "      <td>2090000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7134</th>\n",
       "      <td>881000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20353</th>\n",
       "      <td>900000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22996</th>\n",
       "      <td>1650000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30964</th>\n",
       "      <td>477000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26667</th>\n",
       "      <td>1275000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24774</th>\n",
       "      <td>1240000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10172</th>\n",
       "      <td>1421000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26962</th>\n",
       "      <td>1500000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33899</th>\n",
       "      <td>615000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2336</th>\n",
       "      <td>471000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6110</th>\n",
       "      <td>2081000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12294</th>\n",
       "      <td>1760000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29185</th>\n",
       "      <td>430000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16779</th>\n",
       "      <td>4750000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7094</th>\n",
       "      <td>801000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16079</th>\n",
       "      <td>675000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15619</th>\n",
       "      <td>1155000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25039</th>\n",
       "      <td>711000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1778 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Price\n",
       "2367   1330000.0\n",
       "20408   980000.0\n",
       "19819  2515000.0\n",
       "8833    680000.0\n",
       "1753   2905000.0\n",
       "5390    706750.0\n",
       "20901   730000.0\n",
       "9831    910000.0\n",
       "34542   922000.0\n",
       "10782   501000.0\n",
       "18844  2400000.0\n",
       "10383  1435000.0\n",
       "29844  1700000.0\n",
       "2215   1800000.0\n",
       "24725  1310000.0\n",
       "9460   1016000.0\n",
       "5135    400000.0\n",
       "26147  6125000.0\n",
       "1884   3900000.0\n",
       "14220   360000.0\n",
       "12579   305000.0\n",
       "9067    450000.0\n",
       "32702  1300000.0\n",
       "3035    585000.0\n",
       "7632   1196000.0\n",
       "18398  1420000.0\n",
       "10816  1350000.0\n",
       "18716   755000.0\n",
       "15331  1350000.0\n",
       "262     580000.0\n",
       "...          ...\n",
       "6163    490000.0\n",
       "10961  1450000.0\n",
       "3101   1577500.0\n",
       "3514   1172000.0\n",
       "12138   480000.0\n",
       "1433   1415000.0\n",
       "17636   450000.0\n",
       "2122   1813000.0\n",
       "27330  1560000.0\n",
       "17791  1635000.0\n",
       "32161   856000.0\n",
       "26146  2090000.0\n",
       "7134    881000.0\n",
       "20353   900000.0\n",
       "22996  1650000.0\n",
       "30964   477000.0\n",
       "26667  1275000.0\n",
       "24774  1240000.0\n",
       "10172  1421000.0\n",
       "26962  1500000.0\n",
       "33899   615000.0\n",
       "2336    471000.0\n",
       "6110   2081000.0\n",
       "12294  1760000.0\n",
       "29185   430000.0\n",
       "16779  4750000.0\n",
       "7094    801000.0\n",
       "16079   675000.0\n",
       "15619  1155000.0\n",
       "25039   711000.0\n",
       "\n",
       "[1778 rows x 1 columns]"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display actual result\n",
    "pd.DataFrame(ytest*1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_192 (Dense)            (None, 12)                3984      \n",
      "_________________________________________________________________\n",
      "dense_193 (Dense)            (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dense_194 (Dense)            (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dense_195 (Dense)            (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dense_196 (Dense)            (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dense_197 (Dense)            (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dense_198 (Dense)            (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_199 (Dense)            (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_200 (Dense)            (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_201 (Dense)            (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 4,981\n",
      "Trainable params: 4,981\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(model, to_file='depnet.png')\n",
    "# SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
